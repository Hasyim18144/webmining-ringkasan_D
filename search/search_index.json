{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"CRAWLING/","text":"../README.md CRAWLING \u00b6 Pengertian web crawler- atau sering juga disebut spiders adalah sebuah tool untuk mengindeks dan mengunduh konten dari internet, lalu disimpan kedalam database mesin pencarian. Beberapa web crawler lain selain Googlebot adalah sebagi berikut : 1. Bingbot dari Bing 2. Slurp Bot dari Yahoo 3. DuckDuckBot dari DuckDuckGO 4. Baiduspider dari Baidu (mesin pencari dari cina) 5. Yandex Bot dari Yandex (mesin pencari dari rusia) 6. Sogou Spider dari Sogou (mesin pencari dari cina) 7. Exabot dari Exalead 8. Alexa Crawler dari Amazona Bagaimana Cara Kerja Crawler? Internet selalu berubah dan berkembang setiap waktunya. Karena tak memungkinkan untuk mengetahui jumlah pasti beberapa banyak halaman yang ada di internet, web crawler ini memulai pekerjaannya berdasarkan daftar link halaman yang sudah iya kenal sebelumnya dari sitemap suatu website. Dari daftar link sitemap tersebut, ia akan menemukan link-link lain yaang tersebar di dalamnya. Setelah itu, ia akan melakukan crawling ke link-link yang baru saja ditemukan itu. Proses ini akan terulang lagi di link selanjutnya dan bisa terus berjalan tanpa henti. Namun web crawler ini tidak sembarangan melakukan crawling. Ada beberapa aturan yang tetap harus mereka patuhi, sehingga merka bisa lebih selektif dalam crawling. Biasanya dalam melakukan crawling, ia mempertimbangkan tiga hal yaitu : Seberapa penting dan relavan suatu halaman Kunjungan rutin Menuruti keinginan Robots.txt Robots.txt ini merupakan file di sebuah website yang berisi informasi mengenai halaman mana yang boleh diindeks dan halaman mana yang tidak boleh. Fungsi Web Crawler Fungsi utama dari web crawler memang mengindeks konten di internet. Namun disamping itu, ada beberapa fungsi lain yang juga tidak kalah penting : Membandingkan Harga Data untuk Tools Analisis Data untuk Statistik Crawling Data Twitter Menggunakan Python untuk crawling data twitter kita akan menggunakan library Tweepy . untuk menginstall Tweepy ada tiga cara : Via pip \u200b pip install tweepy Via clone repository dari github \u200b git clone https://github.com/tweepy/tweepy.git \u200b cd tweepy \u200b pip install install langsung dari repository github \u200b pip install git+https://github.com/tweepy/tweepy.git Saya merekomendasikan menggunakan pip, karena lebih simple. selanjutnya kita bisa melihat source code dibawah ini. import tweepy import csvaccess_token=\"\" access_token_secret=\"\" consumer_key=\"\" consumer_key_secret=\"\"auth = tweepy.OAuthHandler(consumer_key,consumer_key_secret) api = tweepy.API(auth)# Open/create a file to append data to csvFile = open('nama-file.csv', 'w', encoding='utf-8')#Use csv writer csvWriter = csv.writer(csvFile)for tweet in tweepy.Cursor(api.search, q='#Python -filter:retweets', tweet_mode='extended',lang=\"id\", since='2021-01-01', until='2021-01-10').items(100): text = tweet.full_text user = tweet.user.name created = tweet.created_at csvWriter.writerow([created, text.encode('utf-8'), user]) csvWriter = csv.writer(csvFile) csvFile.close() Kesimpulan untuk sebuah tool yang bekerja dibalik layar tanpa henti, web crawler ini memberikan banyak manfaat, bukan ? setelah mengetahui banyak manfaatnya, anda pasti menginginkan web crawler mengindeks ke website anda. Unrtuk membuat web crawler mengindeks website anda, maka anda perlu mengoptimasi website anda. Baik dari aspek SEO, desain, hingga responsivitas website anda. Referensi https://www.niagahoster.co.id/blog/apa-itu-web-crawler/ https://blog.javan.co.id/crawling-data-twitter-menggunakan-python-postman-cffbf14f962c","title":"Crawl"},{"location":"CRAWLING/#crawling","text":"Pengertian web crawler- atau sering juga disebut spiders adalah sebuah tool untuk mengindeks dan mengunduh konten dari internet, lalu disimpan kedalam database mesin pencarian. Beberapa web crawler lain selain Googlebot adalah sebagi berikut : 1. Bingbot dari Bing 2. Slurp Bot dari Yahoo 3. DuckDuckBot dari DuckDuckGO 4. Baiduspider dari Baidu (mesin pencari dari cina) 5. Yandex Bot dari Yandex (mesin pencari dari rusia) 6. Sogou Spider dari Sogou (mesin pencari dari cina) 7. Exabot dari Exalead 8. Alexa Crawler dari Amazona Bagaimana Cara Kerja Crawler? Internet selalu berubah dan berkembang setiap waktunya. Karena tak memungkinkan untuk mengetahui jumlah pasti beberapa banyak halaman yang ada di internet, web crawler ini memulai pekerjaannya berdasarkan daftar link halaman yang sudah iya kenal sebelumnya dari sitemap suatu website. Dari daftar link sitemap tersebut, ia akan menemukan link-link lain yaang tersebar di dalamnya. Setelah itu, ia akan melakukan crawling ke link-link yang baru saja ditemukan itu. Proses ini akan terulang lagi di link selanjutnya dan bisa terus berjalan tanpa henti. Namun web crawler ini tidak sembarangan melakukan crawling. Ada beberapa aturan yang tetap harus mereka patuhi, sehingga merka bisa lebih selektif dalam crawling. Biasanya dalam melakukan crawling, ia mempertimbangkan tiga hal yaitu : Seberapa penting dan relavan suatu halaman Kunjungan rutin Menuruti keinginan Robots.txt Robots.txt ini merupakan file di sebuah website yang berisi informasi mengenai halaman mana yang boleh diindeks dan halaman mana yang tidak boleh. Fungsi Web Crawler Fungsi utama dari web crawler memang mengindeks konten di internet. Namun disamping itu, ada beberapa fungsi lain yang juga tidak kalah penting : Membandingkan Harga Data untuk Tools Analisis Data untuk Statistik Crawling Data Twitter Menggunakan Python untuk crawling data twitter kita akan menggunakan library Tweepy . untuk menginstall Tweepy ada tiga cara : Via pip \u200b pip install tweepy Via clone repository dari github \u200b git clone https://github.com/tweepy/tweepy.git \u200b cd tweepy \u200b pip install install langsung dari repository github \u200b pip install git+https://github.com/tweepy/tweepy.git Saya merekomendasikan menggunakan pip, karena lebih simple. selanjutnya kita bisa melihat source code dibawah ini. import tweepy import csvaccess_token=\"\" access_token_secret=\"\" consumer_key=\"\" consumer_key_secret=\"\"auth = tweepy.OAuthHandler(consumer_key,consumer_key_secret) api = tweepy.API(auth)# Open/create a file to append data to csvFile = open('nama-file.csv', 'w', encoding='utf-8')#Use csv writer csvWriter = csv.writer(csvFile)for tweet in tweepy.Cursor(api.search, q='#Python -filter:retweets', tweet_mode='extended',lang=\"id\", since='2021-01-01', until='2021-01-10').items(100): text = tweet.full_text user = tweet.user.name created = tweet.created_at csvWriter.writerow([created, text.encode('utf-8'), user]) csvWriter = csv.writer(csvFile) csvFile.close() Kesimpulan untuk sebuah tool yang bekerja dibalik layar tanpa henti, web crawler ini memberikan banyak manfaat, bukan ? setelah mengetahui banyak manfaatnya, anda pasti menginginkan web crawler mengindeks ke website anda. Unrtuk membuat web crawler mengindeks website anda, maka anda perlu mengoptimasi website anda. Baik dari aspek SEO, desain, hingga responsivitas website anda. Referensi https://www.niagahoster.co.id/blog/apa-itu-web-crawler/ https://blog.javan.co.id/crawling-data-twitter-menggunakan-python-postman-cffbf14f962c","title":"CRAWLING"},{"location":"MODELING/","text":"MODELING / PEMODELAN \u00b6 Topik Modeling dengan Python \u00b6 Pemodelan topik adalah teknik tanpa pengawasan yang bermaksud untuk menganalisis data teks dalam jumlah besar dengan mengelompokkan dokumen ke dalam kelompok. Dalam kasus pemodelan topik, data teks tidak memiliki label yang melekat padanya. Sebaliknya, pemodelan topik mencoba mengelompokkan dokumen ke dalam kelompok berdasarkan karakteristik yang sama. Dua pendekatan terutama digunakan untuk pemodelan topik : Alokasi Dirichlet Laten dan Faktorisasi Matriks Non-Negatif. Di bagian selanjutnya, kita akan meninjau secara singkat kedua pendekatan ini dan akan melihat bagaimana keduanya dapat diterapkan pada pemodelan topik dengan python. Alokasi Dirichlet Laten (LDA) \u00b6 LDA didasarkan pada dua asumsi umum : Dokumen yang memiliki kata yang mirip biasanya memiliki topik yang sama. Dokumen yang memiliki kelompok kata yang sering muncul bersamaan biasanya memiliki topik yang sama. Secara matematis, kedua asumsi diatas dapat direpresentasikan sebagai : Dokumen adalah distribusi probalitas atas topik laten. Topik adalah distribusi probalitas atas kata-kata. LDA untuk Pemodelan Topik Dengan Python \u00b6 Kumpulan data berisi ulasan pengguna untuk berbagai produk dalam kategori makanan. kami akan menggunakan LDA untuk mengelompokkan ulasan pengguna ke dalam 5 kategori. Langkah pertama, seperti biasa, adalah mengimpor kumpulan data bersama dengan pustaka yang diperlukan. jalankan skrip berikut untuk menjalankan. import pandas as pd import numpy as np reviews_datasets = pd.read_csv(r'E:\\Datasets\\Reviews.csv') reviews_datasets = reviews_datasets.head(20000) reviews_datasets.dropna() Selanjutnya, kami mencetak lima baris pertama dari dataset menggunakan head() fungsi untuk memeriksa data kami. reviews_datasets.head() Pada output, anda akan melihat data berikut: Kami akan menerapkan LDA pada kolom \"Teks\" karena berisi ulasan, kolom lainnya akan diabaikan. Mari kita lihat ulasan nomor 350. reviews_datasets['Text'][350] pada output, Anda akan melihat teks ulasan berikut: 'These chocolate covered espresso beans are wonderful! The chocolate is very dark and rich and the \"bean\" inside is a very delightful blend of flavors with just enough caffine to really give it a zing.' Sebelum kita dapat menerapkan LDA, kita perlu membuat kosakata dari semua kata dalam data lain. Ingat dari atrtikel sebelumnya, kita bisa melakukan denganbantuan vectorizer. perhatikan srip berikut: from sklearn.feature_extraction.text import CountVectorizer count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words='english') doc_term_matrix = count_vect.fit_transform(reviews_datasets['Text'].values.astype('U')) Dalam skrip di atas kami menggunakan CountVectorizer kelas dari sklearn.feature_extraction.text modul untuk membuat matriks istilah dokumen. Sekarang kita lihat matrks istilah dokumen doc_term_matrix Kekurangan : <20000x14546 sparse matrix of type '<class 'numpy.int64'>' with 594703 stored elements in Compressed Sparse Row format> Masing-masing dari 20k dokumen di representasikan sebagai vektor dimensi 14546, yang berarti bahwa kosakata kita memiliki 14546 kata. selanjutnya, kita akan menggunakan LDA untuk membuat topik bersama dengan distribusi probalitas untuk setiap kata dalam kosakata kita untuk setiap topik. Jalankan skrip berikut : from sklearn.decomposition import LatentDirichletAllocation LDA = LatentDirichletAllocation(n_components=5, random_state=42) LDA.fit(doc_term_matrix) skrip berikut secara acak mengambil 10 kata dari kosakata kami: import random for i in range(10): random_id = random.randint(0,len(count_vect.get_feature_names())) print(count_vect.get_feature_names()[random_id]) Outputnya terlihat sepeti ini: bribe tarragon qualifies prepare hangs noted churning breeds zon chunkier Mari kita cari 10 kata dengan probabilitas tertinggi untuk topik pertama. Untuk mendapatkan topik pertama, Anda dapat menggunakan `components_ atribut dan memberikan indeks 0 sebagai nilai: first_topic = LDA.components_[0] Topik pertama berisi probabilitas 14546 kata untuk topik 1. Untuk mengurutkan indeks menurut nilai probabilitas, kita dapat menggunakan `argsort() fungsi. Setelah diurutkan, 10 kata dengan probabilitas tertinggi sekarang akan menjadi 10 indeks terakhir dari array. Skrip berikut mengembalikan indeks dari 10 kata dengan probabilitas tertinggi: top_topic_words = first_topic.argsort()[-10:] Keluaran: array([14106, 5892, 7088, 4290, 12596, 5771, 5187, 12888, 7498, 12921], dtype=int64) Indeks ini kemudian dapat digunakan untuk mengambil nilai kata dari count_vect objek, yang dapat dilakukan seperti ini: for i in top_topic_words: print(count_vect.get_feature_names()[i]) Dalam output, Anda akan melihat kata-kata berikut: water great just drink sugar good flavor taste like tea Kata-kata itu menunjukkan bahwa topik pertama mungkin tentang teh. Mari kita cetak 10 kata dengan probabilitas tertinggi untuk kelima topik tersebut: for i,topic in enumerate(LDA.components_): print(f'Top 10 words for topic #{i}:') print([count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]]) print('\\n') Outputnya terlihat: Top 10 words for topic #0: ['water', 'great', 'just', 'drink', 'sugar', 'good', 'flavor', 'taste', 'like', 'tea'] Top 10 words for topic #1: ['br', 'chips', 'love', 'flavor', 'chocolate', 'just', 'great', 'taste', 'good', 'like'] Top 10 words for topic #2: ['just', 'drink', 'orange', 'sugar', 'soda', 'water', 'like', 'juice', 'product', 'br'] Top 10 words for topic #3: ['gluten', 'eat', 'free', 'product', 'like', 'dogs', 'treats', 'dog', 'br', 'food'] Top 10 words for topic #4: ['cups', 'price', 'great', 'like', 'amazon', 'good', 'br', 'product', 'cup', 'coffee'] Output menunjukkan bahwa topik kedua mungkin berisi ulasan tentang cokelat, dll. Demikian pula, topik ketiga mungkin lagi berisi ulasan tentang soda atau jus. Anda dapat melihat bahwa ada beberapa kata umum di semua kategori. Ini karena ada beberapa kata yang digunakan untuk hampir semua topik. Misalnya \"bagus\", \"hebat\", \"suka\" dll. Sebagai langkah terakhir, kami akan menambahkan kolom ke bingkai data asli yang akan menyimpan topik untuk teks. Untuk melakukannya, kita dapat menggunakan `LDA.transform() metode dan menyebarkan matriks istilah dokumen kita. Metode ini akan menetapkan probabilitas semua topik untuk setiap dokumen. Perhatikan kode berikut: topic_values = LDA.transform(doc_term_matrix) topic_values.shape Pada output, Anda akan melihat (20000, 5) yang berarti bahwa setiap dokumen memiliki 5 kolom di mana setiap kolom sesuai dengan nilai probabilitas topik tertentu. Untuk menemukan indeks topik dengan nilai maksimum, kita dapat memanggil `argmax() metode dan melewatkan 1 sebagai nilai untuk parameter sumbu. Skrip berikut menambahkan kolom baru untuk topik dalam bingkai data dan menetapkan nilai topik ke setiap baris dalam kolom: reviews_datasets['Topic'] = topic_values.argmax(axis=1) Sekarang mari kita lihat bagaimana kumpulan data terlihat: reviews_datasets.head() Keluaran: Anda dapat melihat kolom baru untuk topik di output. Referensi : https://stackabuse.com/python-for-nlp-topic-modeling","title":"Modeling"},{"location":"MODELING/#modeling-pemodelan","text":"","title":"MODELING / PEMODELAN"},{"location":"MODELING/#topik-modeling-dengan-python","text":"Pemodelan topik adalah teknik tanpa pengawasan yang bermaksud untuk menganalisis data teks dalam jumlah besar dengan mengelompokkan dokumen ke dalam kelompok. Dalam kasus pemodelan topik, data teks tidak memiliki label yang melekat padanya. Sebaliknya, pemodelan topik mencoba mengelompokkan dokumen ke dalam kelompok berdasarkan karakteristik yang sama. Dua pendekatan terutama digunakan untuk pemodelan topik : Alokasi Dirichlet Laten dan Faktorisasi Matriks Non-Negatif. Di bagian selanjutnya, kita akan meninjau secara singkat kedua pendekatan ini dan akan melihat bagaimana keduanya dapat diterapkan pada pemodelan topik dengan python.","title":"Topik Modeling dengan Python"},{"location":"MODELING/#alokasi-dirichlet-laten-lda","text":"LDA didasarkan pada dua asumsi umum : Dokumen yang memiliki kata yang mirip biasanya memiliki topik yang sama. Dokumen yang memiliki kelompok kata yang sering muncul bersamaan biasanya memiliki topik yang sama. Secara matematis, kedua asumsi diatas dapat direpresentasikan sebagai : Dokumen adalah distribusi probalitas atas topik laten. Topik adalah distribusi probalitas atas kata-kata.","title":"Alokasi Dirichlet Laten (LDA)"},{"location":"MODELING/#lda-untuk-pemodelan-topik-dengan-python","text":"Kumpulan data berisi ulasan pengguna untuk berbagai produk dalam kategori makanan. kami akan menggunakan LDA untuk mengelompokkan ulasan pengguna ke dalam 5 kategori. Langkah pertama, seperti biasa, adalah mengimpor kumpulan data bersama dengan pustaka yang diperlukan. jalankan skrip berikut untuk menjalankan. import pandas as pd import numpy as np reviews_datasets = pd.read_csv(r'E:\\Datasets\\Reviews.csv') reviews_datasets = reviews_datasets.head(20000) reviews_datasets.dropna() Selanjutnya, kami mencetak lima baris pertama dari dataset menggunakan head() fungsi untuk memeriksa data kami. reviews_datasets.head() Pada output, anda akan melihat data berikut: Kami akan menerapkan LDA pada kolom \"Teks\" karena berisi ulasan, kolom lainnya akan diabaikan. Mari kita lihat ulasan nomor 350. reviews_datasets['Text'][350] pada output, Anda akan melihat teks ulasan berikut: 'These chocolate covered espresso beans are wonderful! The chocolate is very dark and rich and the \"bean\" inside is a very delightful blend of flavors with just enough caffine to really give it a zing.' Sebelum kita dapat menerapkan LDA, kita perlu membuat kosakata dari semua kata dalam data lain. Ingat dari atrtikel sebelumnya, kita bisa melakukan denganbantuan vectorizer. perhatikan srip berikut: from sklearn.feature_extraction.text import CountVectorizer count_vect = CountVectorizer(max_df=0.8, min_df=2, stop_words='english') doc_term_matrix = count_vect.fit_transform(reviews_datasets['Text'].values.astype('U')) Dalam skrip di atas kami menggunakan CountVectorizer kelas dari sklearn.feature_extraction.text modul untuk membuat matriks istilah dokumen. Sekarang kita lihat matrks istilah dokumen doc_term_matrix Kekurangan : <20000x14546 sparse matrix of type '<class 'numpy.int64'>' with 594703 stored elements in Compressed Sparse Row format> Masing-masing dari 20k dokumen di representasikan sebagai vektor dimensi 14546, yang berarti bahwa kosakata kita memiliki 14546 kata. selanjutnya, kita akan menggunakan LDA untuk membuat topik bersama dengan distribusi probalitas untuk setiap kata dalam kosakata kita untuk setiap topik. Jalankan skrip berikut : from sklearn.decomposition import LatentDirichletAllocation LDA = LatentDirichletAllocation(n_components=5, random_state=42) LDA.fit(doc_term_matrix) skrip berikut secara acak mengambil 10 kata dari kosakata kami: import random for i in range(10): random_id = random.randint(0,len(count_vect.get_feature_names())) print(count_vect.get_feature_names()[random_id]) Outputnya terlihat sepeti ini: bribe tarragon qualifies prepare hangs noted churning breeds zon chunkier Mari kita cari 10 kata dengan probabilitas tertinggi untuk topik pertama. Untuk mendapatkan topik pertama, Anda dapat menggunakan `components_ atribut dan memberikan indeks 0 sebagai nilai: first_topic = LDA.components_[0] Topik pertama berisi probabilitas 14546 kata untuk topik 1. Untuk mengurutkan indeks menurut nilai probabilitas, kita dapat menggunakan `argsort() fungsi. Setelah diurutkan, 10 kata dengan probabilitas tertinggi sekarang akan menjadi 10 indeks terakhir dari array. Skrip berikut mengembalikan indeks dari 10 kata dengan probabilitas tertinggi: top_topic_words = first_topic.argsort()[-10:] Keluaran: array([14106, 5892, 7088, 4290, 12596, 5771, 5187, 12888, 7498, 12921], dtype=int64) Indeks ini kemudian dapat digunakan untuk mengambil nilai kata dari count_vect objek, yang dapat dilakukan seperti ini: for i in top_topic_words: print(count_vect.get_feature_names()[i]) Dalam output, Anda akan melihat kata-kata berikut: water great just drink sugar good flavor taste like tea Kata-kata itu menunjukkan bahwa topik pertama mungkin tentang teh. Mari kita cetak 10 kata dengan probabilitas tertinggi untuk kelima topik tersebut: for i,topic in enumerate(LDA.components_): print(f'Top 10 words for topic #{i}:') print([count_vect.get_feature_names()[i] for i in topic.argsort()[-10:]]) print('\\n') Outputnya terlihat: Top 10 words for topic #0: ['water', 'great', 'just', 'drink', 'sugar', 'good', 'flavor', 'taste', 'like', 'tea'] Top 10 words for topic #1: ['br', 'chips', 'love', 'flavor', 'chocolate', 'just', 'great', 'taste', 'good', 'like'] Top 10 words for topic #2: ['just', 'drink', 'orange', 'sugar', 'soda', 'water', 'like', 'juice', 'product', 'br'] Top 10 words for topic #3: ['gluten', 'eat', 'free', 'product', 'like', 'dogs', 'treats', 'dog', 'br', 'food'] Top 10 words for topic #4: ['cups', 'price', 'great', 'like', 'amazon', 'good', 'br', 'product', 'cup', 'coffee'] Output menunjukkan bahwa topik kedua mungkin berisi ulasan tentang cokelat, dll. Demikian pula, topik ketiga mungkin lagi berisi ulasan tentang soda atau jus. Anda dapat melihat bahwa ada beberapa kata umum di semua kategori. Ini karena ada beberapa kata yang digunakan untuk hampir semua topik. Misalnya \"bagus\", \"hebat\", \"suka\" dll. Sebagai langkah terakhir, kami akan menambahkan kolom ke bingkai data asli yang akan menyimpan topik untuk teks. Untuk melakukannya, kita dapat menggunakan `LDA.transform() metode dan menyebarkan matriks istilah dokumen kita. Metode ini akan menetapkan probabilitas semua topik untuk setiap dokumen. Perhatikan kode berikut: topic_values = LDA.transform(doc_term_matrix) topic_values.shape Pada output, Anda akan melihat (20000, 5) yang berarti bahwa setiap dokumen memiliki 5 kolom di mana setiap kolom sesuai dengan nilai probabilitas topik tertentu. Untuk menemukan indeks topik dengan nilai maksimum, kita dapat memanggil `argmax() metode dan melewatkan 1 sebagai nilai untuk parameter sumbu. Skrip berikut menambahkan kolom baru untuk topik dalam bingkai data dan menetapkan nilai topik ke setiap baris dalam kolom: reviews_datasets['Topic'] = topic_values.argmax(axis=1) Sekarang mari kita lihat bagaimana kumpulan data terlihat: reviews_datasets.head() Keluaran: Anda dapat melihat kolom baru untuk topik di output. Referensi : https://stackabuse.com/python-for-nlp-topic-modeling","title":"LDA untuk Pemodelan Topik Dengan Python"},{"location":"PREPOCESSING/","text":"PREPOCESSING \u00b6 Prepocessing merupakan salah satu tahapan yang penting untuk data pada proses mining. Data yang digunakan dalam proses mining tidak selamanya dalam kondisi ideal untuk diproses. Terkadang pada data tersebut terdapat berbagai permasalahan yang dapat mengganggu hasil dari proses mining itu sendiri seperti diantaranya adalah missing value, data redundant, outliers, ataupun format data yang tidak sesuai dengan sistem. Oleh karenanya untuk mengatasi permasalahan tersebut dibutuhkan tahap prepocessing. Prepocessing merupakan salah satu tahapan menghilangkan permasalahan - permasalahan yang dapat mengganggu hasil daripada proses data. Dalam kasus klasifikasi dokumen yang menggunakan data bertipe teks, terdapat beberapa macam proses yang dilakukan umumnya diantaranya case folding, filtering(remove punctuation), stopword removal, stemming, tokenization dan sebagainya. Langkah-Langkah Proses Prepocessing Data dokumen untuk diproses Representasi Data Cleaning data dengan menghilangkan tanda baca atau karakter selain teks dengan fungsi punctuation removal. Punctuation Removal Case Folding yang merupakan proses untuk merubah setiap kata menjadi sama, misal huruf kecil dengan menggunakan fungsi lowercase. Case Fold Stopword Removal, menghapus kata-kata yang terlalu umum dan kurang penting ciri-ciri pada kata ini adalah frekwensi kemunculannya yang jumlahnya cukup banyak dibandingkan dengan kata yang lainnya. Contoh kata : aku, kamu, dengan, yang dan seterusnya. Stopword Removal Stemming adalah proses untuk mengubah kata pada setiap kalimat ke bentuk dasar atau menghapus kata-kata imbuhan. Stemming Tokenizing merupakan tahap untuk memenggal setiap kata dalam kalimat termasuk kareakter. Tokenizing Itulah contoh dari proses prepocessing text, berikutnya data hasil prepocessing dapat digunakan untuk keperluan proses fitur seleksi, fitur ekstraksi, klasifikasi dan yang lainnya. Program Python Prepocessing Buat file function.py ``` import re #regular expression import xlwt #library untuk membaca data pada excel from nltk.tokenize import word_tokenize #libray untuk tokenizing #Sastrawi Library untuk Stemming dan Stopword Removal Data Set Bahasa Indonesia from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory factori = StemmerFactory() stemmer = factori.create_stemmer() factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() class hadisClass(object): def init (self,hadis,k1,k2,k3): self.hadis = hadis self.k1 = k1 self.k2 = k2 self.k3 = k3 def openFile(wb): hadisContent = [] items= [] for sheet in wb.sheets(): num_row, num_col = sheet.nrows,4 for row in range(num_row): values = [] for col in range(num_col): if col == 0: x = (sheet.cell(row,col).value) value = (sheet.cell(row,col).value) values.append(value) hadisContent.append(x) item = hadisClass(*values) items.append(item) return hadisContent, items def prepro(datahadis): dhadis=[] for i in datahadis: cleanning = re.sub('[^a-zA-z\\s]','', i) casefold = cleanning.lower(); stopW = stopword.remove(casefold) stemming = stemmer.stem(stopW) tokens = word_tokenize(stemming) dhadis.append(tokens) return dhadis ``` Masih dalam folder yang sama buat file testRun.py ``` import function as fc from xlrd import open_workbook wb = open_workbook('book1.xlsx') data, items = fc.openFile(wb) datatest = [] for j in data : datatest.append(j) datatest = fc.prepro(datatest) print(datatest) ``` Jika berhasil bentuk ouput data akan seperti dibawah ini. Referensi https://hendroprasetyo.com/apa-itu-preprocessing/#.YMAquvkzbIV","title":"Prepocessing"},{"location":"PREPOCESSING/#prepocessing","text":"Prepocessing merupakan salah satu tahapan yang penting untuk data pada proses mining. Data yang digunakan dalam proses mining tidak selamanya dalam kondisi ideal untuk diproses. Terkadang pada data tersebut terdapat berbagai permasalahan yang dapat mengganggu hasil dari proses mining itu sendiri seperti diantaranya adalah missing value, data redundant, outliers, ataupun format data yang tidak sesuai dengan sistem. Oleh karenanya untuk mengatasi permasalahan tersebut dibutuhkan tahap prepocessing. Prepocessing merupakan salah satu tahapan menghilangkan permasalahan - permasalahan yang dapat mengganggu hasil daripada proses data. Dalam kasus klasifikasi dokumen yang menggunakan data bertipe teks, terdapat beberapa macam proses yang dilakukan umumnya diantaranya case folding, filtering(remove punctuation), stopword removal, stemming, tokenization dan sebagainya. Langkah-Langkah Proses Prepocessing Data dokumen untuk diproses Representasi Data Cleaning data dengan menghilangkan tanda baca atau karakter selain teks dengan fungsi punctuation removal. Punctuation Removal Case Folding yang merupakan proses untuk merubah setiap kata menjadi sama, misal huruf kecil dengan menggunakan fungsi lowercase. Case Fold Stopword Removal, menghapus kata-kata yang terlalu umum dan kurang penting ciri-ciri pada kata ini adalah frekwensi kemunculannya yang jumlahnya cukup banyak dibandingkan dengan kata yang lainnya. Contoh kata : aku, kamu, dengan, yang dan seterusnya. Stopword Removal Stemming adalah proses untuk mengubah kata pada setiap kalimat ke bentuk dasar atau menghapus kata-kata imbuhan. Stemming Tokenizing merupakan tahap untuk memenggal setiap kata dalam kalimat termasuk kareakter. Tokenizing Itulah contoh dari proses prepocessing text, berikutnya data hasil prepocessing dapat digunakan untuk keperluan proses fitur seleksi, fitur ekstraksi, klasifikasi dan yang lainnya. Program Python Prepocessing Buat file function.py ``` import re #regular expression import xlwt #library untuk membaca data pada excel from nltk.tokenize import word_tokenize #libray untuk tokenizing #Sastrawi Library untuk Stemming dan Stopword Removal Data Set Bahasa Indonesia from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory factori = StemmerFactory() stemmer = factori.create_stemmer() factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() class hadisClass(object): def init (self,hadis,k1,k2,k3): self.hadis = hadis self.k1 = k1 self.k2 = k2 self.k3 = k3 def openFile(wb): hadisContent = [] items= [] for sheet in wb.sheets(): num_row, num_col = sheet.nrows,4 for row in range(num_row): values = [] for col in range(num_col): if col == 0: x = (sheet.cell(row,col).value) value = (sheet.cell(row,col).value) values.append(value) hadisContent.append(x) item = hadisClass(*values) items.append(item) return hadisContent, items def prepro(datahadis): dhadis=[] for i in datahadis: cleanning = re.sub('[^a-zA-z\\s]','', i) casefold = cleanning.lower(); stopW = stopword.remove(casefold) stemming = stemmer.stem(stopW) tokens = word_tokenize(stemming) dhadis.append(tokens) return dhadis ``` Masih dalam folder yang sama buat file testRun.py ``` import function as fc from xlrd import open_workbook wb = open_workbook('book1.xlsx') data, items = fc.openFile(wb) datatest = [] for j in data : datatest.append(j) datatest = fc.prepro(datatest) print(datatest) ``` Jika berhasil bentuk ouput data akan seperti dibawah ini. Referensi https://hendroprasetyo.com/apa-itu-preprocessing/#.YMAquvkzbIV","title":"PREPOCESSING"}]}